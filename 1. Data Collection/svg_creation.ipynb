{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b61cb40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/ankit/Desktop/Portfolio/kaggle/drawing-with-llms\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4a084dc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ast\n",
    "import io\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import statistics\n",
    "import string\n",
    "import time\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List\n",
    "\n",
    "import cairosvg\n",
    "import pandas as pd\n",
    "import torch\n",
    "from camel.agents import ChatAgent\n",
    "from camel.datagen.cot_datagen import CoTDataGenerator, logger\n",
    "from camel.models.model_factory import ModelFactory, ModelPlatformType, ModelType\n",
    "from datasets import load_dataset\n",
    "from defusedxml import ElementTree as etree\n",
    "from dotenv import load_dotenv\n",
    "from more_itertools import chunked\n",
    "from PIL import Image\n",
    "from pydantic import BaseModel\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    AutoProcessor,\n",
    "    BitsAndBytesConfig,\n",
    "    PaliGemmaForConditionalGeneration,\n",
    ")\n",
    "\n",
    "load_dotenv(\".env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a548987a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ModelFactory.create(\n",
    "    model_platform=ModelPlatformType.OPENAI_COMPATIBLE_MODEL,\n",
    "    model_type=ModelType.GEMINI_2_5_FLASH_PREVIEW,\n",
    "    url=\"https://generativelanguage.googleapis.com/v1beta/openai/\",\n",
    "    api_key=os.getenv(\"GEMINI_API_KEY\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9636f727",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class SVGConstraints:\n",
    "    \"\"\"Defines constraints for validating SVG documents.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    max_svg_size : int, default=10000\n",
    "        Maximum allowed size of an SVG file in bytes.\n",
    "    allowed_elements : dict[str, set[str]]\n",
    "        Mapping of the allowed elements to the allowed attributes of each element.\n",
    "    \"\"\"\n",
    "\n",
    "    max_svg_size: int = 10000\n",
    "    allowed_elements: dict[str, set[str]] = field(\n",
    "        default_factory=lambda: {\n",
    "            \"common\": {\n",
    "                \"id\",\n",
    "                \"clip-path\",\n",
    "                \"clip-rule\",\n",
    "                \"color\",\n",
    "                \"color-interpolation\",\n",
    "                \"color-interpolation-filters\",\n",
    "                \"color-rendering\",\n",
    "                \"display\",\n",
    "                \"fill\",\n",
    "                \"fill-opacity\",\n",
    "                \"fill-rule\",\n",
    "                \"filter\",\n",
    "                \"flood-color\",\n",
    "                \"flood-opacity\",\n",
    "                \"lighting-color\",\n",
    "                \"marker-end\",\n",
    "                \"marker-mid\",\n",
    "                \"marker-start\",\n",
    "                \"mask\",\n",
    "                \"opacity\",\n",
    "                \"paint-order\",\n",
    "                \"stop-color\",\n",
    "                \"stop-opacity\",\n",
    "                \"stroke\",\n",
    "                \"stroke-dasharray\",\n",
    "                \"stroke-dashoffset\",\n",
    "                \"stroke-linecap\",\n",
    "                \"stroke-linejoin\",\n",
    "                \"stroke-miterlimit\",\n",
    "                \"stroke-opacity\",\n",
    "                \"stroke-width\",\n",
    "                \"transform\",\n",
    "            },\n",
    "            \"svg\": {\n",
    "                \"width\",\n",
    "                \"height\",\n",
    "                \"viewBox\",\n",
    "                \"preserveAspectRatio\",\n",
    "            },\n",
    "            \"g\": {\"viewBox\"},\n",
    "            \"defs\": set(),\n",
    "            \"symbol\": {\"viewBox\", \"x\", \"y\", \"width\", \"height\"},\n",
    "            \"use\": {\"x\", \"y\", \"width\", \"height\", \"href\"},\n",
    "            \"marker\": {\n",
    "                \"viewBox\",\n",
    "                \"preserveAspectRatio\",\n",
    "                \"refX\",\n",
    "                \"refY\",\n",
    "                \"markerUnits\",\n",
    "                \"markerWidth\",\n",
    "                \"markerHeight\",\n",
    "                \"orient\",\n",
    "            },\n",
    "            \"pattern\": {\n",
    "                \"viewBox\",\n",
    "                \"preserveAspectRatio\",\n",
    "                \"x\",\n",
    "                \"y\",\n",
    "                \"width\",\n",
    "                \"height\",\n",
    "                \"patternUnits\",\n",
    "                \"patternContentUnits\",\n",
    "                \"patternTransform\",\n",
    "                \"href\",\n",
    "            },\n",
    "            \"linearGradient\": {\n",
    "                \"x1\",\n",
    "                \"x2\",\n",
    "                \"y1\",\n",
    "                \"y2\",\n",
    "                \"gradientUnits\",\n",
    "                \"gradientTransform\",\n",
    "                \"spreadMethod\",\n",
    "                \"href\",\n",
    "            },\n",
    "            \"radialGradient\": {\n",
    "                \"cx\",\n",
    "                \"cy\",\n",
    "                \"r\",\n",
    "                \"fx\",\n",
    "                \"fy\",\n",
    "                \"fr\",\n",
    "                \"gradientUnits\",\n",
    "                \"gradientTransform\",\n",
    "                \"spreadMethod\",\n",
    "                \"href\",\n",
    "            },\n",
    "            \"stop\": {\"offset\"},\n",
    "            \"filter\": {\n",
    "                \"x\",\n",
    "                \"y\",\n",
    "                \"width\",\n",
    "                \"height\",\n",
    "                \"filterUnits\",\n",
    "                \"primitiveUnits\",\n",
    "            },\n",
    "            \"feBlend\": {\"result\", \"in\", \"in2\", \"mode\"},\n",
    "            \"feFlood\": {\"result\"},\n",
    "            \"feOffset\": {\"result\", \"in\", \"dx\", \"dy\"},\n",
    "            \"path\": {\"d\"},\n",
    "            \"rect\": {\"x\", \"y\", \"width\", \"height\", \"rx\", \"ry\"},\n",
    "            \"circle\": {\"cx\", \"cy\", \"r\"},\n",
    "            \"ellipse\": {\"cx\", \"cy\", \"rx\", \"ry\"},\n",
    "            \"line\": {\"x1\", \"y1\", \"x2\", \"y2\"},\n",
    "            \"polyline\": {\"points\"},\n",
    "            \"polygon\": {\"points\"},\n",
    "        }\n",
    "    )\n",
    "\n",
    "    def validate_svg(self, svg_code: str) -> None:\n",
    "        \"\"\"Validates an SVG string against a set of predefined constraints.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        svg_code : str\n",
    "            The SVG string to validate.\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        ValueError\n",
    "            If the SVG violates any of the defined constraints.\n",
    "        \"\"\"\n",
    "        # Check file size\n",
    "        if len(svg_code.encode(\"utf-8\")) > self.max_svg_size:\n",
    "            raise ValueError(\"SVG exceeds allowed size\")\n",
    "\n",
    "        # Parse XML\n",
    "        tree = etree.fromstring(\n",
    "            svg_code.encode(\"utf-8\"),\n",
    "            forbid_dtd=True,\n",
    "            forbid_entities=True,\n",
    "            forbid_external=True,\n",
    "        )\n",
    "\n",
    "        elements = set(self.allowed_elements.keys())\n",
    "\n",
    "        # Check elements and attributes\n",
    "        for element in tree.iter():\n",
    "            # Check for disallowed elements\n",
    "            tag_name = element.tag.split(\"}\")[-1]\n",
    "            if tag_name not in elements:\n",
    "                raise ValueError(f\"Disallowed element: {tag_name}\")\n",
    "\n",
    "            # Check attributes\n",
    "            for attr, attr_value in element.attrib.items():\n",
    "                # Check for disallowed attributes\n",
    "                attr_name = attr.split(\"}\")[-1]\n",
    "                if (\n",
    "                    attr_name not in self.allowed_elements[tag_name]\n",
    "                    and attr_name not in self.allowed_elements[\"common\"]\n",
    "                ):\n",
    "                    raise ValueError(f\"Disallowed attribute: {attr_name}\")\n",
    "\n",
    "                # Check for embedded data\n",
    "                if \"data:\" in attr_value.lower():\n",
    "                    raise ValueError(\"Embedded data not allowed\")\n",
    "                if \";base64\" in attr_value:\n",
    "                    raise ValueError(\"Base64 encoded content not allowed\")\n",
    "\n",
    "                # Check that href attributes are internal references\n",
    "                if attr_name == \"href\":\n",
    "                    if not attr_value.startswith(\"#\"):\n",
    "                        raise ValueError(\n",
    "                            f'Invalid href attribute in <{tag_name}>. Only internal references (starting with \"#\") are allowed. Found: \"{attr_value}\"'\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb57c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VQAEvaluator:\n",
    "    def __init__(self):\n",
    "        self.quantization_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_compute_dtype=torch.float16,\n",
    "        )\n",
    "        self.letters = string.ascii_uppercase\n",
    "        # self.model_path = kagglehub.model_download(\n",
    "        #     \"google/paligemma-2/transformers/paligemma2-3b-mix-448\"\n",
    "        # )\n",
    "        self.model_path = \"google/paligemma2-3b-pt-448\"\n",
    "        self.processor = AutoProcessor.from_pretrained(self.model_path)\n",
    "        self.model = PaliGemmaForConditionalGeneration.from_pretrained(\n",
    "            self.model_path,\n",
    "            low_cpu_mem_usage=True,\n",
    "            quantization_config=self.quantization_config,\n",
    "        ).to(\"cuda:1\")  # type: ignore\n",
    "\n",
    "    def score(self, questions, choices, answers, images, n=4):\n",
    "        # TODO change this !\n",
    "        scores = []\n",
    "        batches = (chunked(qs, n) for qs in [questions, choices, answers])\n",
    "        for question_batch, choice_batch, answer_batch in zip(*batches, strict=True):\n",
    "            scores.extend(\n",
    "                self.score_batch(\n",
    "                    images,  # list of images\n",
    "                    question_batch,\n",
    "                    choice_batch,\n",
    "                    answer_batch,\n",
    "                )\n",
    "            )\n",
    "        return statistics.mean(scores)\n",
    "\n",
    "    def score_batch(\n",
    "        self,\n",
    "        images: list[Image.Image],\n",
    "        questions: list[str],\n",
    "        choices_list: list[list[str]],\n",
    "        answers: list[str],\n",
    "    ) -> list[float]:\n",
    "        prompts = [\n",
    "            self.format_prompt(question, choices)\n",
    "            for question, choices in zip(questions, choices_list, strict=True)\n",
    "        ]\n",
    "        batched_choice_probabilities = self.get_choice_probability(\n",
    "            images,  # send list of images\n",
    "            prompts,\n",
    "            choices_list,\n",
    "        )\n",
    "\n",
    "        scores = []\n",
    "        for i, _ in enumerate(questions):\n",
    "            choice_probabilities = batched_choice_probabilities[i]\n",
    "            answer = answers[i]\n",
    "            answer_probability = 0.0\n",
    "            for choice, prob in choice_probabilities.items():\n",
    "                if choice == answer:\n",
    "                    answer_probability = prob\n",
    "                    break\n",
    "            scores.append(answer_probability)\n",
    "\n",
    "        return scores\n",
    "\n",
    "    def format_prompt(self, question: str, choices: list[str]) -> str:\n",
    "        prompt = f\"<image>answer en Question: {question}\\nChoices:\\n\"\n",
    "        for i, choice in enumerate(choices):\n",
    "            prompt += f\"{self.letters[i]}. {choice}\\n\"\n",
    "        return prompt\n",
    "\n",
    "    def mask_choices(self, logits, choices_list):\n",
    "        batch_size = logits.shape[0]\n",
    "        masked_logits = torch.full_like(logits, float(\"-inf\"))\n",
    "\n",
    "        for batch_idx in range(batch_size):\n",
    "            choices = choices_list[batch_idx]\n",
    "            for i in range(len(choices)):\n",
    "                letter_token = self.letters[i]\n",
    "\n",
    "                first_token = self.processor.tokenizer.encode(\n",
    "                    letter_token, add_special_tokens=False\n",
    "                )[0]\n",
    "                first_token_with_space = self.processor.tokenizer.encode(\n",
    "                    \" \" + letter_token, add_special_tokens=False\n",
    "                )[0]\n",
    "\n",
    "                if isinstance(first_token, int):\n",
    "                    masked_logits[batch_idx, first_token] = logits[\n",
    "                        batch_idx, first_token\n",
    "                    ]\n",
    "                if isinstance(first_token_with_space, int):\n",
    "                    masked_logits[batch_idx, first_token_with_space] = logits[\n",
    "                        batch_idx, first_token_with_space\n",
    "                    ]\n",
    "\n",
    "        return masked_logits\n",
    "\n",
    "    def get_choice_probability(self, images, prompts, choices_list) -> list[dict]:\n",
    "        inputs = self.processor(\n",
    "            # images=[image] * len(prompts),\n",
    "            images=images,  # batch different images\n",
    "            text=prompts,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=\"longest\",\n",
    "        ).to(\"cuda:1\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "            logits = outputs.logits[:, -1, :]  # Logits for the last (predicted) token\n",
    "            masked_logits = self.mask_choices(logits, choices_list)\n",
    "            probabilities = torch.softmax(masked_logits, dim=-1)\n",
    "\n",
    "        batched_choice_probabilities = []\n",
    "        for batch_idx in range(len(prompts)):\n",
    "            choice_probabilities = {}\n",
    "            choices = choices_list[batch_idx]\n",
    "            for i, choice in enumerate(choices):\n",
    "                letter_token = self.letters[i]\n",
    "                first_token = self.processor.tokenizer.encode(\n",
    "                    letter_token, add_special_tokens=False\n",
    "                )[0]\n",
    "                first_token_with_space = self.processor.tokenizer.encode(\n",
    "                    \" \" + letter_token, add_special_tokens=False\n",
    "                )[0]\n",
    "\n",
    "                prob = 0.0\n",
    "                if isinstance(first_token, int):\n",
    "                    prob += probabilities[batch_idx, first_token].item()\n",
    "                if isinstance(first_token_with_space, int):\n",
    "                    prob += probabilities[batch_idx, first_token_with_space].item()\n",
    "                choice_probabilities[choice] = prob\n",
    "\n",
    "            # Renormalize probabilities for each question\n",
    "            total_prob = sum(choice_probabilities.values())\n",
    "            if total_prob > 0:\n",
    "                renormalized_probabilities = {\n",
    "                    choice: prob / total_prob\n",
    "                    for choice, prob in choice_probabilities.items()\n",
    "                }\n",
    "            else:\n",
    "                renormalized_probabilities = (\n",
    "                    choice_probabilities  # Avoid division by zero if total_prob is 0\n",
    "                )\n",
    "            batched_choice_probabilities.append(renormalized_probabilities)\n",
    "\n",
    "        return batched_choice_probabilities\n",
    "\n",
    "    def ocr(self, images, free_chars=4):\n",
    "        inputs = (\n",
    "            self.processor(\n",
    "                text=[\"<image>ocr\\n\"] * len(images),\n",
    "                images=images,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "            .to(torch.float16)\n",
    "            .to(self.model.device)\n",
    "        )\n",
    "        input_len = inputs[\"input_ids\"].shape[-1]\n",
    "\n",
    "        with torch.inference_mode():\n",
    "            outputs = self.model.generate(**inputs, max_new_tokens=32, do_sample=False)\n",
    "            out_list = self.processor.batch_decode(\n",
    "                outputs[:, input_len:], skip_special_tokens=True\n",
    "            )\n",
    "\n",
    "        scores = [1.0 if len(decoded) < free_chars else -1.0 for decoded in out_list]\n",
    "\n",
    "        return scores\n",
    "\n",
    "\n",
    "def svg_to_png(svg_code: str, size: tuple = (384, 384)):\n",
    "    if \"viewBox\" not in svg_code:\n",
    "        svg_code = svg_code.replace(\"<svg\", f'<svg viewBox=\"0 0 {size[0]} {size[1]}\"')\n",
    "\n",
    "    png_data = cairosvg.svg2png(bytestring=svg_code.encode(\"utf-8\"))\n",
    "    return Image.open(io.BytesIO(png_data)).convert(\"RGB\").resize(size)  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6acc1419",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT = \"\"\" \n",
    "# Role\n",
    "You are a meticulous and highly skilled AI SVG Architect. Your primary function is to translate rich textual descriptions of sceneries into precise, well-structured SVG code.\n",
    "\n",
    "# Objective\n",
    "Generate SVG code that accurately and comprehensively depicts the scenery described in the input text. The generated SVG will undergo rigorous Visual Question Answering (VQA) evaluation. All VQA questions will be answerable *solely* from the information present in the original text description. Therefore, absolute fidelity to *all* details in the description is paramount.\n",
    "\n",
    "# Strategy\n",
    "Generating complex SVG directly is prone to errors. Employ a Chain of Thought (CoT) process to first decompose the described scenery into constituent entities, map these entities to fundamental SVG primitives (rectangles, circles, paths, etc.), plan their attributes and layout, and only then construct the final SVG code. This structured approach is critical for accuracy and for adhering to the constraints.\n",
    "\n",
    "# Constraints\n",
    "1.  Allowed SVG Elements: `svg`, `path`, `circle`, `rect`, `ellipse`, `line`, `polyline`, `polygon`, `g`, `linearGradient`, `radialGradient`, `stop`, `defs`.\n",
    "2.  Allowed SVG Attributes: `viewBox`, `width`, `height`, `fill`, `stroke`, `stroke-width`, `d`, `cx`, `cy`, `r`, `x`, `y`, `rx`, `ry`, `x1`, `y1`, `x2`, `y2`, `points`, `transform`, `opacity`. *No other elements or attributes are permitted.*\n",
    "3.  Token Limit: Your entire response (CoT + SVG code) must not exceed 2000 tokens. Conciseness in both your reasoning and the generated SVG is key.\n",
    "4.  Completeness: The SVG must visually represent *every* distinct object, property, and spatial relationship mentioned in the description.\n",
    "\n",
    "Chain of Thought (CoT) Steps:\n",
    "\n",
    "Phase 1: Scene Deconstruction & Entity Analysis\n",
    "    1.1. Full Text Parsing: Read and internalize the entire description.\n",
    "    1.2. Entity Identification: List every distinct visual entity mentioned (e.g., sun, specific tree type, house, car model if specified, clouds).\n",
    "    1.3. Attribute Extraction per Entity: For each entity, meticulously detail its properties:\n",
    "        *   Component Primitives: Identify how the entity can be constructed from one or more allowed SVG primitives (e.g., a \"house\" might be a `rect` for the body and a `polygon` or `path` for the roof). Be explicit about this decomposition.\n",
    "        *   Visual Properties: Color (fill, stroke), stroke width, opacity. Specify exact color values (e.g., \"blue\", \"#FF0000\", \"rgb(0,0,255)\"). If a color is implied (e.g., \"a grassy field\"), infer a common color (e.g., green).\n",
    "        *   Size & Scale: Note any described dimensions or relative sizes (e.g., \"a tall tree,\" \"a small window\"). If not specified, use reasonable default proportions relative to other objects or the canvas.\n",
    "        *   Position & Orientation: Note absolute (e.g., \"in the top-left corner\") or relative positioning (e.g., \"the sun is above the mountains,\" \"a car is parked next to the house\"). Also note any rotation or skew if described and how it might be achieved with `transform`.\n",
    "        *   Relationships:** Document how entities relate to each other spatially (e.g., overlapping, adjacent, contained within).\n",
    "\n",
    "Phase 2: SVG Mapping & Layout Planning\n",
    "    2.1. Canvas Definition: Choose viewBox=\"0 0 368 368\". This defines your coordinate space. All subsequent coordinates will be relative to this.\n",
    "    2.2. Primitive Mapping & Attribute Specification: For each component primitive identified in 1.3:\n",
    "        *   Select the precise SVG elements\n",
    "        *   Translate the visual properties from 1.3 into specific SVG attribute values (e.g., `fill=\"blue\"`, `r=\"10\"`, `d=\"...\"`).\n",
    "        *   Calculate and assign coordinates (`cx`, `cy`, `x`, `y`, `points`, path commands) and dimensions (`r`, `width`, `height`) based on the `viewBox` and the entity's position/size from 1.3. Be explicit about these calculations if they are not trivial.\n",
    "        *   Consider the z-ordering (drawing order): elements drawn later appear on top. Plan the sequence of elements accordingly (e.g., background elements first, foreground elements last).\n",
    "    2.3. Grouping Strategy: Determine if `<g>` elements are beneficial for grouping components of complex entities or for applying shared transformations or styles. Plan any `transform` attributes for these groups or individual elements.\n",
    "    2.4. Gradient Definitions: If the description implies gradients (e.g., \"sky fading from blue to orange\"), define `linearGradient` or `radialGradient` elements within `<defs>` with appropriate `stop` colors and offsets. Assign them unique `id`s. These `id`s will be referenced in `fill` attributes (e.g., `fill=\"url(#myGradient)\"`).\n",
    "\n",
    "Phase 3: Code Generation \n",
    "    3.1. SVG Construction: Systematically write the SVG code.\n",
    "\n",
    "# Output Format\n",
    "### COT\n",
    "[Chain of Thought as a multi line single paragraph]\n",
    "### SVG\n",
    "[SVG Code]\n",
    "\n",
    "### Input\n",
    "Description: {description}\n",
    "\n",
    "### Output\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6282dd2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatAgentResponse(msgs=[BaseMessage(role_name='assistant', role_type=<RoleType.ASSISTANT: 'assistant'>, meta_dict={}, content='Worlds drawn bright,\\nHeroes take flight.\\nStories unfold, brave, bold.\\nEmotions shown,\\nMagic known.\\nAnime dreams,\\nVibrant gleams.', video_bytes=None, image_list=None, image_detail='auto', video_detail='low', parsed=None)], terminated=False, info={'id': 'n584aNSMMoe1kdUPr4CR-Ag', 'usage': {'prompt_tokens': 10, 'completion_tokens': 36, 'total_tokens': 638}, 'termination_reasons': ['stop'], 'num_tokens': 15, 'tool_calls': [], 'external_tool_call_requests': None})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class VQACoT(CoTDataGenerator):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.vqa_model = VQAEvaluator()\n",
    "        self.dataset = load_dataset(\n",
    "            \"csv\",\n",
    "            data_files=\"data/descriptions_with_vqa.csv\",\n",
    "        )[\"train\"]  # type: ignore\n",
    "        self.default_svg = \"\"\"<svg width=\"256\" height=\"256\" viewBox=\"0 0 256 256\"><circle cx=\"50\" cy=\"50\" r=\"40\" fill=\"red\" /></svg>\"\"\"\n",
    "        self.svg_constraints = SVGConstraints()\n",
    "\n",
    "    def extract_response(self, text: str):\n",
    "        text = text.split(\"### COT\")[1]\n",
    "        cot, svg = text.split(\"### SVG\")\n",
    "\n",
    "        if \"```svg\" in svg:\n",
    "            svg = svg.split(\"```svg\")[1]\n",
    "            svg = svg.split(\"```\")[0]\n",
    "\n",
    "        return cot.strip(), svg.strip()\n",
    "\n",
    "    def extract_image(self, text: str):\n",
    "        matches = re.findall(r\"<svg.*?</svg>\", text, re.DOTALL | re.IGNORECASE)\n",
    "        if matches:\n",
    "            svg = matches[-1]\n",
    "        else:\n",
    "            svg = self.default_svg\n",
    "\n",
    "        try:\n",
    "            self.svg_constraints.validate_svg(svg)\n",
    "        except Exception:\n",
    "            svg = self.default_svg\n",
    "\n",
    "        image = svg_to_png(svg)\n",
    "\n",
    "        return image\n",
    "\n",
    "    def get_answer(self, question: str, context: str = \"\"):\n",
    "        self.generator_agent.reset()\n",
    "        response = self.generator_agent.step(PROMPT.format(question))  # description\n",
    "        cot, svg = self.extract_response(response.msgs[0].content)\n",
    "        answer = cot + \"\\n\" + svg\n",
    "        logger.info(\"AI thought process:\\n%s\", answer)\n",
    "        return answer\n",
    "\n",
    "    def verify_answer(self, question: str, answer: str):\n",
    "        sample = self.dataset[self.dataset[\"description\"].index(question)]  # type: ignore\n",
    "        limit = 4\n",
    "        question_list = ast.literal_eval(sample[\"question\"])[:limit]\n",
    "        choices_list = ast.literal_eval(sample[\"choices\"])[:limit]\n",
    "        answer_list = ast.literal_eval(sample[\"answer\"])[:limit]\n",
    "\n",
    "        image = self.extract_image(answer)\n",
    "\n",
    "        score = self.vqa_model.score(\n",
    "            question_list, choices_list, answer_list, [image], 1\n",
    "        )  # batch size 1\n",
    "\n",
    "        if score > 0.8:  # Threshold for acceptance\n",
    "            is_correct = True\n",
    "        else:\n",
    "            is_correct = False\n",
    "\n",
    "        logger.info(\"Answer verification result: %s\", is_correct)\n",
    "        return is_correct\n",
    "\n",
    "    def evaluate_partial_solution(self, question: str, partial_solution: str = \"\"):\n",
    "        sample = self.dataset[self.dataset[\"description\"].index(question)]  # type: ignore\n",
    "        limit = 4\n",
    "        question_list = ast.literal_eval(sample[\"question\"])[:limit]\n",
    "        choices_list = ast.literal_eval(sample[\"choices\"])[:limit]\n",
    "        answer_list = ast.literal_eval(sample[\"answer\"])[:limit]\n",
    "\n",
    "        image = self.extract_image(partial_solution)\n",
    "\n",
    "        score = self.vqa_model.score(\n",
    "            question_list, choices_list, answer_list, [image], 1\n",
    "        )  # batch size 1\n",
    "        return score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ff2d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : Run with GPU"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "auto",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
