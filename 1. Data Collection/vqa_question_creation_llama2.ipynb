{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c21f248",
   "metadata": {},
   "source": [
    "## Use Fine Tuned Llama2 from TIFA Benchmark Team to Generate Question and Answers for VQA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19559500",
   "metadata": {},
   "source": [
    "acknowledgement \n",
    "- https://www.kaggle.com/code/richolson/tifa-question-generation-qwen-vs-tuned-llama/notebook\n",
    "- https://github.com/Yushi-Hu/tifa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d70adc",
   "metadata": {},
   "source": [
    "Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03e7db38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import transformers\n",
    "import torch\n",
    "import gc\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add4d084",
   "metadata": {},
   "source": [
    "Load pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7064561",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"tifa-benchmark/llama2_tifa_question_generation\"\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"tifa-benchmark/llama2_tifa_question_generation\",\n",
    "    torch_dtype=torch.bfloat16,  # Use half precision for memory efficiency\n",
    "    device_map=\"auto\",\n",
    "    low_cpu_mem_usage=True,\n",
    ")\n",
    "\n",
    "llama_pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611ba119",
   "metadata": {},
   "source": [
    "Follow TIFA Prompt format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d17a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_qg_prompt(caption):\n",
    "    INTRO_BLURB = \"Given an image description, generate one or two multiple-choice questions that verifies if the image description is correct.\\nClassify each concept into a type (object, human, animal, food, activity, attribute, counting, color, material, spatial, location, shape, other), and then generate a question for each type.\\n\"\n",
    "    formated_prompt = f\"<s>[INST] <<SYS>>\\n{INTRO_BLURB}\\n<</SYS>>\\n\\n\"\n",
    "    formated_prompt += f\"Description: {caption} [/INST] Entities:\"\n",
    "    return formated_prompt\n",
    "\n",
    "\n",
    "test_caption = \"a purple forest at dusk\"\n",
    "\n",
    "prompt = create_qg_prompt(test_caption)\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc489cfb",
   "metadata": {},
   "source": [
    "Create TIFA VQA Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05fe426",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_questions(output_text):\n",
    "    questions = []\n",
    "    q_pattern = re.compile(r\"Q: (.*?)\\nChoices: (.*?)\\nA: (.*?)(?=\\n\\w|$)\", re.DOTALL)\n",
    "    matches = q_pattern.findall(output_text)\n",
    "\n",
    "    for match in matches:\n",
    "        question = match[0].strip()\n",
    "        choices_str = match[1].strip()\n",
    "        answer = match[2].strip()\n",
    "\n",
    "        choices = [choice.strip() for choice in choices_str.split(\",\")]\n",
    "\n",
    "        questions.append({\"question\": question, \"choices\": choices, \"answer\": answer})\n",
    "\n",
    "    return questions\n",
    "\n",
    "\n",
    "def process_with_model(description, pipeline):\n",
    "    prompt = create_qg_prompt(description)\n",
    "\n",
    "    try:\n",
    "        sequences = pipeline(\n",
    "            prompt, do_sample=True, max_length=512, num_return_sequences=1\n",
    "        )\n",
    "\n",
    "        full_output = sequences[0][\"generated_text\"]\n",
    "        output_text = full_output[len(prompt) :]\n",
    "\n",
    "        # handle whitepsaces, extract first part\n",
    "        if \"\\n\\n\" in output_text:\n",
    "            output_text = output_text.split(\"\\n\\n\")[0]\n",
    "\n",
    "        # Extract questions\n",
    "        questions = extract_questions(output_text)\n",
    "\n",
    "        list_questions = [q[\"question\"] for q in questions]\n",
    "        list_choices = [q[\"choices\"] for q in questions]\n",
    "        list_answer = [q[\"answer\"] for q in questions]\n",
    "\n",
    "        return list_questions, list_choices, list_answer\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing: {e}\")\n",
    "        return [], [], []\n",
    "\n",
    "\n",
    "def create_dataset(df, pipeline):\n",
    "    description_collection = []\n",
    "    question_collection = []\n",
    "    choices_collection = []\n",
    "    answer_collection = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        description = row[\"description\"]\n",
    "\n",
    "        print(f\"Processing: {description}\")\n",
    "\n",
    "        # Process with model through the pipeline\n",
    "        list_questions, list_choices, list_answer = process_with_model(\n",
    "            description, pipeline\n",
    "        )\n",
    "\n",
    "        description_collection.append(description)\n",
    "        question_collection.append(list_questions)\n",
    "        choices_collection.append(list_choices)\n",
    "        answer_collection.append(list_answer)\n",
    "\n",
    "        print(f\"  Extracted {len(list_questions)} questions\")\n",
    "        # Clean up memory\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    if question_collection:\n",
    "        results_df = pd.DataFrame(\n",
    "            {\n",
    "                \"description\": description_collection,\n",
    "                \"question\": question_collection,\n",
    "                \"choices\": choices_collection,\n",
    "                \"answer\": answer_collection,\n",
    "            }\n",
    "        )\n",
    "        return results_df\n",
    "    else:\n",
    "        print(\"No results generated\")\n",
    "        return None\n",
    "\n",
    "\n",
    "description_df = pd.read_csv(\"data/descriptions.csv\")[:5]\n",
    "tifa_llama_predicted_questions = create_dataset(description_df, llama_pipeline)\n",
    "# tifa_llama_predicted_questions.to_csv(\"data/descriptions_with_vqa.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "auto",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
